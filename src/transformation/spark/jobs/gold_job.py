"""
JOB GOLD - Transforma√ß√£o Silver ‚Üí Gold
=======================================

O QUE FAZ A CAMADA GOLD?
------------------------
A Gold √© a camada de "resposta". Ela:
1. Cruza diferentes fontes de dados (clima + culturas)
2. Calcula m√©tricas de neg√≥cio (risco clim√°tico)
3. Gera tabelas prontas para dashboards
4. RESPONDE PERGUNTAS DE NEG√ìCIO

PERGUNTA QUE VAMOS RESPONDER:
-----------------------------
"Qual o risco clim√°tico de cada cultura por esta√ß√£o do ano?"

ANALOGIA: √â o prato pronto servido na mesa.
Bronze = ingredientes organizados
Silver = ingredientes preparados
Gold = refei√ß√£o pronta para comer

CONCEITOS SPARK USADOS:
-----------------------
1. crossJoin() - Cruza todas as linhas de duas tabelas (produto cartesiano)
2. UDF - User Defined Function (fun√ß√£o Python dentro do Spark)
3. Broadcast - Otimiza√ß√£o para tabelas pequenas
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, when, lit, current_timestamp, udf, broadcast,
    round as spark_round, concat, desc, asc, row_number
)
from pyspark.sql.types import (
    IntegerType, StringType, FloatType, StructType, StructField
)
from pyspark.sql.window import Window
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from src.transformation.spark.spark_session import criar_spark_session


# =========================================================
# FUN√á√ïES DE C√ÅLCULO DE RISCO
# =========================================================
# Estas fun√ß√µes implementam a l√≥gica de neg√≥cio

def calcular_score_temperatura_udf():
    """
    Cria uma UDF (User Defined Function) para calcular o score de temperatura.
    
    O QUE √â UDF?
    ------------
    UDF permite usar fun√ß√µes Python dentro do Spark.
    O Spark distribui essa fun√ß√£o para todos os workers.
    
    CUIDADO: UDFs s√£o mais lentas que fun√ß√µes nativas do Spark.
    Use apenas quando n√£o h√° alternativa nativa.
    
    L√ìGICA DO SCORE:
    ----------------
    - 100: Temperatura perfeita (dentro do range ideal)
    - 70: Fora do ideal, mas toler√°vel (at√© 3¬∞C de diferen√ßa)
    - 45: Moderadamente fora (3-6¬∞C de diferen√ßa)
    - 20: Severamente fora (>6¬∞C de diferen√ßa)
    """
    def calcular(temp_real, temp_min_ideal, temp_max_ideal):
        if temp_real is None or temp_min_ideal is None or temp_max_ideal is None:
            return 50  # Indefinido
        
        # Dentro do range ideal
        if temp_min_ideal <= temp_real <= temp_max_ideal:
            return 95
        
        # Fora do range - calcular diferen√ßa
        if temp_real < temp_min_ideal:
            diferenca = temp_min_ideal - temp_real
        else:
            diferenca = temp_real - temp_max_ideal
        
        # Classificar por severidade
        if diferenca <= 3:
            return 70  # Leve
        elif diferenca <= 6:
            return 45  # Moderado
        else:
            return 20  # Severo
    
    return udf(calcular, IntegerType())


def calcular_score_precipitacao_udf():
    """
    Cria UDF para calcular score de precipita√ß√£o.
    
    L√ìGICA:
    -------
    Compara precipita√ß√£o real com necessidade da cultura.
    - Alta necessidade: precisa 200-500mm por esta√ß√£o
    - M√©dia necessidade: precisa 100-350mm
    - Baixa necessidade: precisa 30-200mm
    """
    def calcular(precip_mm, necessidade_agua, dias_chuva):
        if precip_mm is None or necessidade_agua is None:
            return 50
        
        # Definir ranges ideais por necessidade
        ranges = {
            "Alta": {"min": 200, "max": 500, "critico": 100},
            "M√©dia": {"min": 100, "max": 350, "critico": 50},
            "Baixa": {"min": 30, "max": 200, "critico": 15}
        }
        
        ref = ranges.get(necessidade_agua, ranges["M√©dia"])
        
        # Dentro do ideal
        if ref["min"] <= precip_mm <= ref["max"]:
            if dias_chuva and dias_chuva >= 15:
                return 95  # Bem distribu√≠do
            return 80  # Ok
        
        # D√©ficit cr√≠tico
        if precip_mm < ref["critico"]:
            return 15
        
        # D√©ficit moderado
        if precip_mm < ref["min"]:
            return 50
        
        # Excesso
        if precip_mm > ref["max"]:
            excesso = precip_mm - ref["max"]
            if excesso > 300:
                return 30
            return 60
        
        return 50
    
    return udf(calcular, IntegerType())


def classificar_risco_udf():
    """
    Classifica o risco geral baseado nos scores.
    
    PESOS:
    ------
    - Temperatura: 60%
    - Precipita√ß√£o: 40%
    
    CLASSIFICA√á√ÉO:
    --------------
    - >= 80: BAIXO (verde)
    - >= 60: M√âDIO (amarelo)
    - >= 40: ALTO (laranja)
    - < 40: CR√çTICO (vermelho)
    """
    def classificar(score_temp, score_precip, tolerante_seca, tolerante_frio, sensivel_frio, sensivel_calor):
        if score_temp is None or score_precip is None:
            return "INDEFINIDO"
        
        # Calcular score base (60% temp, 40% precip)
        score_base = score_temp * 0.6 + score_precip * 0.4
        
        # Ajustes por toler√¢ncia
        ajuste = 0
        if tolerante_seca and score_precip < 50:
            ajuste += 15
        if tolerante_frio and score_temp < 60:
            ajuste += 15
        if sensivel_frio and score_temp < 70:
            ajuste -= 20
        if sensivel_calor and score_temp < 70:
            ajuste -= 20
        
        score_final = min(100, max(0, score_base + ajuste))
        
        if score_final >= 80:
            return "BAIXO"
        elif score_final >= 60:
            return "M√âDIO"
        elif score_final >= 40:
            return "ALTO"
        else:
            return "CR√çTICO"
    
    return udf(classificar, StringType())


def gerar_recomendacao_udf():
    """
    Gera recomenda√ß√£o textual baseada no risco.
    """
    def recomendar(risco):
        recomendacoes = {
            "BAIXO": "‚úÖ IDEAL PARA PLANTIO",
            "M√âDIO": "‚ö†Ô∏è POSS√çVEL COM CUIDADOS",
            "ALTO": "‚ö†Ô∏è ALTO RISCO - EVITAR",
            "CR√çTICO": "‚ùå N√ÉO RECOMENDADO",
            "INDEFINIDO": "‚ùì DADOS INSUFICIENTES"
        }
        return recomendacoes.get(risco, "‚ùì DADOS INSUFICIENTES")
    
    return udf(recomendar, StringType())


# =========================================================
# FUN√á√ïES PRINCIPAIS
# =========================================================

def gerar_fato_risco_climatico(
    spark: SparkSession,
    clima_estacional_path: str,
    culturas_path: str,
    output_path: str
) -> DataFrame:
    """
    GERA A TABELA FATO DE RISCO CLIM√ÅTICO
    =====================================
    
    Esta √© a tabela principal que responde:
    "Qual o risco clim√°tico de cada cultura por esta√ß√£o do ano?"
    
    COMO FUNCIONA:
    --------------
    1. Carrega clima por esta√ß√£o (4 linhas: Ver√£o, Outono, Inverno, Primavera)
    2. Carrega culturas (76 culturas)
    3. Faz CROSS JOIN: cada cultura √ó cada esta√ß√£o = 304 combina√ß√µes
    4. Calcula scores de risco para cada combina√ß√£o
    5. Classifica e gera recomenda√ß√µes
    
    CROSS JOIN EXPLICADO:
    ---------------------
    √â como fazer todas as combina√ß√µes poss√≠veis.
    
    Culturas:     Esta√ß√µes:        Resultado:
    Caf√©          Ver√£o            Caf√© + Ver√£o
    Milho    √ó    Inverno     =    Caf√© + Inverno
    Soja          ...              Milho + Ver√£o
                                   Milho + Inverno
                                   ... (304 linhas)
    """
    print("\n" + "="*70)
    print("ü•á GOLD: Gerando FATO_RISCO_CLIMATICO")
    print("="*70)
    print("\nüìå Pergunta: 'Qual o risco clim√°tico de cada cultura por esta√ß√£o?'")
    
    # =========================================================
    # PASSO 1: CARREGAR DADOS
    # =========================================================
    print("\nüìñ Carregando dados Silver...")
    
    df_clima = spark.read.parquet(clima_estacional_path)
    df_culturas = spark.read.parquet(culturas_path)
    
    print(f"   Clima estacional: {df_clima.count()} registros (esta√ß√µes)")
    print(f"   Culturas: {df_culturas.count()} registros")
    
    # Mostrar dados de entrada
    print("\nüå°Ô∏è Clima por Esta√ß√£o:")
    df_clima.show()
    
    print("\nüå± Amostra de Culturas:")
    df_culturas.select(
        "cultura", "categoria", "temp_min_ideal", "temp_max_ideal",
        "necessidade_de_agua", "tolerante_seca"
    ).show(5)
    
    # =========================================================
    # PASSO 2: CROSS JOIN
    # =========================================================
    # Cruzar cada cultura com cada esta√ß√£o
    # broadcast() otimiza quando uma tabela √© pequena (cabe na mem√≥ria)
    
    print("\nüîÄ Fazendo CROSS JOIN (culturas √ó esta√ß√µes)...")
    
    df_cruzado = df_culturas.crossJoin(broadcast(df_clima))
    
    print(f"   Combina√ß√µes geradas: {df_cruzado.count()}")
    
    # =========================================================
    # PASSO 3: CALCULAR SCORES
    # =========================================================
    print("\nüìä Calculando scores de risco...")
    
    # Registrar UDFs
    calc_temp = calcular_score_temperatura_udf()
    calc_precip = calcular_score_precipitacao_udf()
    
    df_scores = df_cruzado \
        .withColumn(
            "score_temperatura",
            calc_temp(
                col("temp_media"),
                col("temp_min_ideal"),
                col("temp_max_ideal")
            )
        ) \
        .withColumn(
            "score_precipitacao",
            calc_precip(
                col("precipitacao_total_mm"),
                col("necessidade_de_agua"),
                col("dias_chuva")
            )
        )
    
    # =========================================================
    # PASSO 4: CLASSIFICAR RISCO
    # =========================================================
    print("\nüéØ Classificando risco geral...")
    
    classif_risco = classificar_risco_udf()
    gerar_recom = gerar_recomendacao_udf()
    
    df_risco = df_scores \
        .withColumn(
            "risco_geral",
            classif_risco(
                col("score_temperatura"),
                col("score_precipitacao"),
                col("tolerante_seca"),
                col("tolerante_frio"),
                col("sensivel_frio"),
                col("sensivel_calor")
            )
        ) \
        .withColumn(
            "recomendacao",
            gerar_recom(col("risco_geral"))
        )
    
    # =========================================================
    # PASSO 5: CALCULAR SCORE FINAL
    # =========================================================
    df_final = df_risco.withColumn(
        "score_final",
        spark_round(col("score_temperatura") * 0.6 + col("score_precipitacao") * 0.4, 0).cast(IntegerType())
    )
    
    # =========================================================
    # PASSO 6: SELECIONAR E ORDENAR COLUNAS
    # =========================================================
    print("\nüìã Organizando colunas finais...")
    
    df_gold = df_final.select(
        # Identificadores
        col("cultura"),
        col("categoria"),
        col("ano"),
        col("estacao"),
        
        # Dados clim√°ticos reais
        col("temp_media").alias("temp_real"),
        col("temp_min").alias("temp_min_real"),
        col("temp_max").alias("temp_max_real"),
        col("precipitacao_total_mm").alias("precipitacao_mm"),
        col("dias_chuva"),
        col("umidade_media"),
        
        # Requisitos da cultura
        col("temp_min_ideal"),
        col("temp_max_ideal"),
        col("necessidade_de_agua"),
        
        # Toler√¢ncias
        col("tolerante_seca"),
        col("tolerante_frio"),
        col("sensivel_frio"),
        col("sensivel_calor"),
        
        # Scores calculados
        col("score_temperatura"),
        col("score_precipitacao"),
        col("score_final"),
        
        # Classifica√ß√£o final
        col("risco_geral"),
        col("recomendacao")
    ).withColumn("processado_em", current_timestamp())
    
    # =========================================================
    # PASSO 7: SALVAR
    # =========================================================
    print(f"\nüíæ Salvando em: {output_path}")
    
    df_gold.write \
        .mode("overwrite") \
        .partitionBy("estacao") \
        .parquet(output_path)
    
    print("\n‚úÖ FATO_RISCO_CLIMATICO gerada com sucesso!")
    
    return df_gold


def gerar_relatorio_resumido(spark: SparkSession, gold_path: str, output_path: str) -> DataFrame:
    """
    Gera um relat√≥rio resumido de risco por esta√ß√£o.
    
    OBJETIVO:
    ---------
    Criar uma vis√£o consolidada para dashboard executivo.
    
    M√âTRICAS:
    ---------
    - Top 10 culturas recomendadas por esta√ß√£o
    - Distribui√ß√£o de risco por categoria
    """
    print("\n" + "="*70)
    print("üìä GOLD: Gerando RELAT√ìRIO RESUMIDO")
    print("="*70)
    
    # Ler dados
    df_gold = spark.read.parquet(gold_path)
    
    # =========================================================
    # RELAT√ìRIO 1: TOP 10 CULTURAS POR ESTA√á√ÉO
    # =========================================================
    print("\nüèÜ TOP 10 CULTURAS POR ESTA√á√ÉO:")
    
    # Usar Window para rankear dentro de cada esta√ß√£o
    window_spec = Window.partitionBy("estacao").orderBy(desc("score_final"))
    
    df_ranking = df_gold \
        .withColumn("ranking", row_number().over(window_spec)) \
        .filter(col("ranking") <= 10) \
        .select(
            "estacao", "ranking", "cultura", "categoria",
            "score_final", "risco_geral", "recomendacao"
        )
    
    # Mostrar por esta√ß√£o
    for estacao in ["Ver√£o", "Outono", "Inverno", "Primavera"]:
        print(f"\n{'='*50}")
        print(f"üåø {estacao.upper()}")
        print('='*50)
        df_ranking.filter(col("estacao") == estacao).show(10, truncate=False)
    
    # =========================================================
    # RELAT√ìRIO 2: DISTRIBUI√á√ÉO DE RISCO
    # =========================================================
    print("\nüìà DISTRIBUI√á√ÉO DE RISCO POR ESTA√á√ÉO:")
    
    df_distribuicao = df_gold.groupBy("estacao", "risco_geral") \
        .count() \
        .orderBy("estacao", "risco_geral")
    
    df_distribuicao.show()
    
    # Salvar relat√≥rios
    df_ranking.write.mode("overwrite").parquet(f"{output_path}/ranking_culturas")
    df_distribuicao.write.mode("overwrite").parquet(f"{output_path}/distribuicao_risco")
    
    return df_ranking


def exibir_resposta_pergunta(spark: SparkSession, gold_path: str):
    """
    EXIBE A RESPOSTA FINAL √Ä PERGUNTA DE NEG√ìCIO
    =============================================
    
    Pergunta: "Qual o risco clim√°tico de cada cultura por esta√ß√£o do ano?"
    """
    print("\n")
    print("‚ïî" + "‚ïê"*70 + "‚ïó")
    print("‚ïë" + " "*20 + "üéØ RESPOSTA √Ä PERGUNTA DE NEG√ìCIO" + " "*17 + "‚ïë")
    print("‚ï†" + "‚ïê"*70 + "‚ï£")
    print("‚ïë Pergunta: Qual o risco clim√°tico de cada cultura por esta√ß√£o?       ‚ïë")
    print("‚ïö" + "‚ïê"*70 + "‚ïù")
    
    df = spark.read.parquet(gold_path)
    
    # Resumo por esta√ß√£o
    print("\nüìä RESUMO EXECUTIVO:")
    print("-"*70)
    
    for estacao in ["Ver√£o", "Outono", "Inverno", "Primavera"]:
        df_est = df.filter(col("estacao") == estacao)
        
        total = df_est.count()
        baixo = df_est.filter(col("risco_geral") == "BAIXO").count()
        medio = df_est.filter(col("risco_geral") == "M√âDIO").count()
        alto = df_est.filter(col("risco_geral") == "ALTO").count()
        critico = df_est.filter(col("risco_geral") == "CR√çTICO").count()
        
        pct_favoravel = (baixo / total) * 100 if total > 0 else 0
        
        emoji = "üåû" if estacao == "Ver√£o" else "üçÇ" if estacao == "Outono" else "‚ùÑÔ∏è" if estacao == "Inverno" else "üå∏"
        
        print(f"\n{emoji} {estacao.upper()}")
        print(f"   ‚úÖ Risco BAIXO:    {baixo:3d} culturas ({baixo/total*100:.0f}%)")
        print(f"   ‚ö†Ô∏è  Risco M√âDIO:   {medio:3d} culturas ({medio/total*100:.0f}%)")
        print(f"   ‚ö†Ô∏è  Risco ALTO:    {alto:3d} culturas ({alto/total*100:.0f}%)")
        print(f"   ‚ùå Risco CR√çTICO: {critico:3d} culturas ({critico/total*100:.0f}%)")
    
    # Top 5 culturas mais vers√°teis (baixo risco em mais esta√ß√µes)
    print("\n" + "="*70)
    print("üèÜ TOP 5 CULTURAS MAIS VERS√ÅTEIS (menor risco geral):")
    print("="*70)
    
    df.filter(col("risco_geral") == "BAIXO") \
        .groupBy("cultura", "categoria") \
        .count() \
        .orderBy(desc("count")) \
        .show(5)
    
    # Culturas mais arriscadas
    print("\n‚ö†Ô∏è CULTURAS QUE EXIGEM MAIS ATEN√á√ÉO (mais esta√ß√µes com risco alto/cr√≠tico):")
    print("="*70)
    
    df.filter(col("risco_geral").isin("ALTO", "CR√çTICO")) \
        .groupBy("cultura", "categoria") \
        .count() \
        .orderBy(desc("count")) \
        .show(5)


# =========================================================
# EXECU√á√ÉO PRINCIPAL
# =========================================================
if __name__ == "__main__":
    spark = criar_spark_session("gold-layer-job")
    
    BASE_PATH = "/home/claude/datalake"
    
    # Paths Silver
    SILVER_CLIMA_ESTACIONAL = f"{BASE_PATH}/silver/clima_estacional"
    SILVER_CULTURAS = f"{BASE_PATH}/silver/culturas"
    
    # Paths Gold
    GOLD_FATO_RISCO = f"{BASE_PATH}/gold/fato_risco_climatico"
    GOLD_RELATORIOS = f"{BASE_PATH}/gold/relatorios"
    
    try:
        # Gerar tabela fato principal
        df_risco = gerar_fato_risco_climatico(
            spark,
            SILVER_CLIMA_ESTACIONAL,
            SILVER_CULTURAS,
            GOLD_FATO_RISCO
        )
        
        # Mostrar amostra
        print("\nüìã AMOSTRA DA TABELA FATO_RISCO_CLIMATICO:")
        df_risco.select(
            "cultura", "estacao", "temp_real", "temp_min_ideal", "temp_max_ideal",
            "score_temperatura", "score_precipitacao", "score_final",
            "risco_geral", "recomendacao"
        ).orderBy("cultura", "estacao").show(20, truncate=False)
        
        # Gerar relat√≥rios
        gerar_relatorio_resumido(spark, GOLD_FATO_RISCO, GOLD_RELATORIOS)
        
        # Exibir resposta final
        exibir_resposta_pergunta(spark, GOLD_FATO_RISCO)
        
        print("\n" + "="*70)
        print("‚úÖ GOLD LAYER COMPLETA!")
        print("="*70)
        print(f"\nüìÅ Arquivos gerados em: {BASE_PATH}/gold/")
        
    finally:
        spark.stop()
